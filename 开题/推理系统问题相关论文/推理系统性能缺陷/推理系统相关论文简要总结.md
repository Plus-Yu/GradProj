## Fast and Efficient Model Serving Using Multi-GPUs with Direct-Host-Access [3552326.3567508]

问题：
如果相应模型已在 GPU 内存中，DL 推理请求可以立即得到处理。否则，就需要将模型从主机加载到 GPU，这会显著增加推理延迟。

解决：
本文提出了 DeepPlan，旨在服务器环境中，在将 DL 模型从主机部署到 GPU 的过程中，最大限度地减少推理延迟。首先，我们利用商用 GPU 提供的直接主机访问功能，使 GPU 无需加载即可直接访问主机内存中特定的模型层。其次，我们通过多个 GPU 并行传输模型，以减少从主机到 GPU 加载模型的时间。

## Tabi: An Efficient Multi-Level Inference System for Large Language Models [3552326.3587438]

问题：
构建规模越来越大的语言模型（LLMs）会增加推理阶段的延迟

解决：
由于给大语言模型增加参数的收益逐渐递减，对于大多数查询而言，较小的模型能够做出与成本高昂的大语言模型相同的预测。基于这一观察，我们设计了 Tabi，这是一个带有多级推理引擎的推理系统，它使用小型模型处理查询，并在有高要求的应用场景中选择性地使用大语言模型。

## SMILE: A Cost-Effective System for Serving Massive Pretrained Language Models in The Cloud [3555041.3589720]

问题：
资源成本高昂：预训练语言模型资源消耗大，需要昂贵硬件支持，传统按租户、按模型的服务策略成本极高
硬件利用率低：多个推理请求对应不同模型时，物理机内存可能无法容纳所有模型，需将模型卸载到外部存储，导致推理速度受 I/O 限制；同时，异构请求难以批量处理，GPU 只能逐个运行，利用率低下

解决：
通过有效的资源共享和复用，提出了一种经济高效的存储和计算方案协同设计，用于在硬件资源受限的情况下管理大量定制的 PLM

## Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models [3567955.3567959]

问题：
设备内存对于这些模型的规模来说过于有限，层内模型并行性是一种解决这些问题的方法。但层内模型并行性所产生的数据通信可能会在总执行时间中占比很大，严重损害计算效率

解决：
将通信与计算重叠来有效降低其数据通信开销。将原本的通信集合操作和相关计算操作分解成更细粒度的操作，使它们可以并行执行，以此创造更多重叠机会，隐藏数据传输延迟，提高系统利用率。

## STI: Turbocharge NLP Inference at the Edge via Elastic Pipelining [3575693.3575698]

问题：
NLP 模型前所未有的规模给延迟和内存都带来了压力，在移动设备的这两种关键资源之间造成了矛盾。为了满足目标延迟，将整个模型保存在内存中可以尽快启动执行，但会使一个应用程序的内存占用增加数倍，在被移动内存管理回收之前，它只能进行少数几次推理，限制了其优势。另一方面，按需从存储中加载模型会导致长达几秒的 IO 时间，远远超出了用户可接受的延迟范围；按层流水线式地加载和执行模型也无法隐藏 IO 延迟，这是因为 IO 和计算延迟之间存在很大差异。

解决：
第一，模型分片。STI 将模型参数作为可独立调整的分片进行管理，并分析它们对准确性的重要程度。第二，带预加载缓冲区的弹性流水线规划。STI 实例化一个 IO / 计算流水线，并使用一个小缓冲区预加载分片，以便在早期阶段无延迟地启动执行；它根据分片的重要性明智地选择、调整和组合分片，以实现资源弹性执行，最大化推理准确性

## Clover: Toward Sustainable AI with Carbon-Aware Machine Learning Inference Service [3581784.3607034]

问题：
数据中心的碳排放占全球碳排放的2%，推理服务占数据中心计算周期的大部分；在减少碳排放的同时，需要保持推理服务的高性能（低延迟）和高准确

解决：
使用不同质量（参数数量和计算复杂度）的模型变体来平衡准确性和碳排放；利用多实例GPU（MIG）技术，允许多个推理服务实例共享同一个物理GPU，从而提高资源利用率；根据实时碳强度动态调整模型变体和GPU分区配置

## Proteus: A High-Throughput Inference-Serving System with Accuracy Scaling [3617232.3624849]

问题：
在边缘集群或私有云等资源受限的环境中，硬件扩展可能不可行，导致系统无法满足高吞吐量需求；在资源受限的情况下，推理系统需要在满足SLO（服务等级目标）的同时，最大化系统精度；需要能够快速适应查询负载的变化

解决：
通过调整模型精度而不是依赖硬件资源来满足变化的查询需求；通过联合优化模型选择、模型放置和查询分配，动态调整系统资源分配，以在满足吞吐量需求的同时最大化系统精度

## Model Selection for Latency-Critical Inference Serving [3627703.3629565]

问题：
现有的模型选择和调度方法过于保守，仅基于查询负载进行模型选择，忽略了查询到达模式的随机性，导致未能充分利用模型精度

解决：
利用马尔可夫决策过程（MDP）对查询负载和到达模式进行建模，并生成在给定延迟约束下最大化精度的策略

## Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention [atc24-gao-bin-cost]

问题：
现有的LLM服务引擎在处理多轮对话时效率低下，原因在于每次对话轮次结束后，引擎会丢弃与该轮次相关的KV缓存，以便为其他活跃会话释放高带宽存储器（HBM）空间。当会话重新激活时，引擎需要重新计算整个KV缓存，导致大量重复计算，浪费了宝贵的GPU计算资源

解决：
在对话会话不活跃时保存KV缓存，并在会话重新激活时复用这些缓存，避免了重复计算；通过分层预加载和异步保存方案，将KV缓存的加载与推理计算重叠

## Power-aware Deep Learning Model Serving with μ-Serve [atc24-qiu]

问题：
在推理阶段，占用了大量能源，且未充分利用GPU频率调整的节能机会

解决：
通过细粒度的模型管理和动态GPU频率调整，在满足性能要求的同时，显著降低了模型服务的能耗

## Quant-LLM: Accelerating the Serving of Large Language Models via FP6-Centric Algorithm-System Co-Design on Modern GPUs [atc24-xia]

问题：
模型量化是一种常见的优化手段，通过减少表示每个模型权重所需的比特数来降低内存占用和加速推理；现有的量化方法主要支持4位和8位量化，而6位量化在推理成本和模型质量之间提供了更好的权衡：模型权重的非2的幂次位宽（如6位）导致的内存访问效率低下；将量化后的权重转换回原始格式需要复杂的位操作

解决：
Quant-LLM通过提前位级预打包技术，优化权重的内存布局，使得权重可以以32位对齐的方式访问；通过优化的SIMT核心指令和并行反量化技术，显著减少了反量化过程中的计算开销

## DeepSpeed-Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale [DeepSpeed-_Inference_Enabling_Efficient_Inference_of_Transformer_Models_at_Unprecedented_Scale]

问题：
在资源受限的环境中，推理时面临着延迟敏感和吞吐量要求的挑战

解决：
Deep-Fusion：通过融合多个操作减少内核调用和数据移动开销。
定制 GeMM 核心：针对小批量情况优化内存带宽利用率。
张量并行和流水线并行：通过张量切片和流水线阶段划分，利用多个 GPU 的聚合内存带宽。
MoE 模型优化：通过专家并行、数据并行和张量并行的组合，以及通信优化策略，实现大规模稀疏模型的高效推理。
ZeRO-Inference：通过将模型权重存储在 CPU 或 NVMe 内存中，并按需流式传输到 GPU 内存中，实现大规模模型的推理。

## High Performance and Power Efficient Accelerator for Cloud Inference

问题：
性能瓶颈：随着DNN模型复杂度增加，推理任务面临计算和数据密集型工作负载，导致推理延迟高、吞吐量受限。
能效问题：高性能推理需要大量计算资源，但传统加速器在高负载下能耗较高，影响性价比。
硬件资源利用不足：多任务和多租户场景下，硬件资源分配不合理，导致资源浪费或性能下降。
灵活性不足：现有推理加速器难以适应快速发展的DNN模型结构和多样化的推理需求。
数据访问瓶颈：推理过程中数据访问频繁，尤其是对于大规模模型，内存带宽和容量成为瓶颈。

解决：
计算引擎：强大的矢量/矩阵引擎，支持多种矩阵形状和数据类型，优化了线性代数运算（如矩阵乘法和卷积）。
内存层次结构：三级内存层次结构，包括大容量本地寄存器、分布式多端口共享内存和高带宽 HBM2E 芯片。
数据流与同步：支持多种数据流和同步模式，优化了计算和内存访问的并行性。
资源抽象与多任务支持：通过硬件资源隔离和抽象，支持多任务和多租户部署。
能效管理：动态功耗完整性和效率管理，优化系统能效。

## PipeInfer: Accelerating LLM Inference using Asynchronous Pipelined Speculation [SC41406.2024.00046]

问题：
现有的推测执行技术虽然可以缓解带宽瓶颈，但在低推测接受率情况下会导致延迟增加

解决：
异步推测（Asynchronous Speculation）：
通过将目标模型和推测模型的计算流水线分离，使两者可以并行运行。这种方式显著降低了首次生成延迟（Time-to-First-Token, TTFT）和令牌间延迟（Inter-Token Latency, ITL），同时提高了系统利用率。
连续推测（Continuous Speculation）：
在等待目标模型完成推理时，推测模型持续生成小批量的推测树，而不是一次性生成一个大批量的推测树。这种方法减少了因推测接受率低而导致的延迟，并提高了系统在低带宽场景下的适应性。
流水线 KV 缓存多缓冲（Pipelined KV Cache Multibuffering）：
为了管理多个推测运行和非推测推理之间的 KV 缓存，PipeInfer 将 KV 缓存序列分割成多个私有部分，每个推测运行分配一个独立的缓存分区。这种方式允许推测运行跳过与之前运行共享的令牌的计算，从而提高了计算吞吐量。
早期推理取消（Early Inference Cancellation）：
当某些推测运行被确定为无效或多余时，PipeInfer 通过反向传播一个取消信号来从流水线中移除这些运行。这一机制减少了因连续推测而导致的性能损失，尤其是在推测模型与目标模型对齐不佳的情况下。

## ParvaGPU: Efficient Spatial GPU Sharing for Large-Scale DNN Inference in Cloud Environments [SC41406.2024.00048]

问题：
在云环境中，现有的 GPU 资源分配方法在满足深度神经网络（DNN）推理服务的延迟要求（SLO）时，存在 GPU 资源浪费的问题，主要表现为内部松弛（GPU 分区内未充分利用）和外部碎片化（跨 GPU 的空间碎片化）

解决：
ParvaGPU 通过结合 NVIDIA 的 MIG 和 MPS 技术，引入了 GPU 分段配置器和分配器，优化了 GPU 资源的分配和调度，从而在满足 SLO 的同时显著减少了 GPU 使用量，降低了内部松弛和外部碎片化

## Optimizing Distributed ML Communication with Fused Computation-Collective Operations [SC41406.2024.00094]

问题：
分布式机器学习（ML）模型在多节点部署时，集体通信（如权重更新、层间激活交换）成为性能瓶颈，尤其是缺乏独立的粗粒度计算内核来执行通信操作，导致通信难以隐藏，显著增加了延迟

解决：
通过 GPU 的大规模并行性和 GPU 主导的通信技术，将通信与计算重叠，同时引入零拷贝融合内核和通信感知调度策略，显著降低了通信开销并提高了分布式 ML 推理系统的效率














